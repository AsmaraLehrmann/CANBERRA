{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d518608",
   "metadata": {},
   "source": [
    "# Welcome to the 210Pb age model script!\n",
    "\n",
    "### <div style=\"text-align: right\"> Last modified by A.A. Lehrmann 18 November 2024 </div>\n",
    "\n",
    "\n",
    "### The script below will extract radioisotope data from Canberra PDFs, run the age model (from the Wellner Lab Group excel model (Appleby, 2001; Boldt et al., 2013), and plot the age model\n",
    "\n",
    "### Important instructions before you begin:\n",
    "\n",
    "    1. NEVER edit raw data. Do not delete Canberra PDFs. Do not remove sediment weights from original lab notebook excel sheet.\n",
    "\n",
    "    2. Make an /CORE_AgeModelOutput/ folder to put all of your script's outputs\n",
    "\n",
    "    3. When copying folder paths, make sure to remove quotation marks\n",
    "\n",
    "    4. Always add the extension .csv to your output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a513b53",
   "metadata": {},
   "source": [
    "## First, extract radioisotope data from Canberra PDFs\n",
    "Run cell (press triangle that says run) and follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937424bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_values(folder_path, output_csv_path, parse_numbers=False):\n",
    "    combined_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".PDF\") or filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Initialize variables to store isotopes' values\n",
    "            pb210 = pb210error = Bi214 = Bi214error = Pb214 = Pb214error = None\n",
    "\n",
    "            # Read in the PDF file\n",
    "            try:\n",
    "                reader = PdfReader(file_path)\n",
    "                if len(reader.pages) < 3:\n",
    "                    print(f\"PDF file '{filename}' has less than 3 pages. Skipping.\")\n",
    "                    continue\n",
    "                page = reader.pages[2]\n",
    "                text = page.extract_text()\n",
    "                lines = text.split('\\n')\n",
    "\n",
    "                # Check if the filename starts with 'PtSrc_'\n",
    "                if filename.startswith(\"PtSrc_\"):\n",
    "                    # Only extract Pb-210 value for PtSrc files\n",
    "                    for line in lines:\n",
    "                        if 'Pb-210' in line:\n",
    "                            ptsrc_pb210, PtSrc_Pb210error = line.split()[-2:]\n",
    "                            break\n",
    "                    # Store only Pb-210 values if found\n",
    "                    if ptsrc_pb210 is not None and PtSrc_Pb210error is not None:\n",
    "                        data = {'File': filename, 'Pb-210': float(ptsrc_pb210), 'Pb-210 error': float(PtSrc_Pb210error)}\n",
    "                    else:\n",
    "                        print(f\"Pb-210 not found in '{filename}'. Skipping.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Extract values for other files\n",
    "                    for line in lines:\n",
    "                        if 'Pb-210' in line:\n",
    "                            pb210, pb210error = line.split()[-2:]\n",
    "                        elif 'Bi-214' in line:\n",
    "                            Bi214, Bi214error = line.split()[-2:]\n",
    "                        elif 'Pb-214' in line:\n",
    "                            Pb214, Pb214error = line.split()[-2:]\n",
    "\n",
    "                    # Validate that all required data is present\n",
    "                    if pb210 is None or pb210error is None:\n",
    "                        print(f\"Pb-210 not found in '{filename}'. Skipping.\")\n",
    "                        continue\n",
    "                    if Bi214 is None or Bi214error is None:\n",
    "                        print(f\"Bi-214 not found in '{filename}'. Skipping.\")\n",
    "                        continue\n",
    "                    if Pb214 is None or Pb214error is None:\n",
    "                        print(f\"Pb-214 not found in '{filename}'. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Store the extracted data\n",
    "                    data = {\n",
    "                        'File': filename,\n",
    "                        'Pb-210': float(pb210),\n",
    "                        'Pb-210 error': float(pb210error),\n",
    "                        'Bi-214': float(Bi214),\n",
    "                        'Bi-214 error': float(Bi214error),\n",
    "                        'Pb-214': float(Pb214),\n",
    "                        'Pb-214 error': float(Pb214error),\n",
    "                    }\n",
    "                \n",
    "                combined_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDF file '{filename}': {e}\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Extract the numeric part at the end of the file name for sorting\n",
    "    def extract_numeric_suffix(file_name):\n",
    "        try:\n",
    "            # Split by underscores, dots, or hyphens and get the last numeric part before the extension\n",
    "            parts = file_name.split('_')[-1].split('.')[0]\n",
    "            return int(parts)\n",
    "        except ValueError:\n",
    "            return float('nan')  # Return NaN if the suffix is not numeric\n",
    "\n",
    "    # Apply sorting based on the numeric suffix\n",
    "    combined_df['File_order'] = combined_df['File'].apply(extract_numeric_suffix)\n",
    "    \n",
    "    # Sort by the numeric part extracted\n",
    "    combined_df = combined_df.sort_values(by='File_order')\n",
    "\n",
    "    # Remove the 'File_order' column before saving\n",
    "    combined_df = combined_df.drop(columns=['File_order'])\n",
    "\n",
    "    # Optional: Parse numbers into floats if specified\n",
    "    if parse_numbers:\n",
    "        for col in ['Pb-210', 'Bi-214', 'Pb-214']:\n",
    "            if col in combined_df.columns:\n",
    "                combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "\n",
    "    # Save the sorted DataFrame to CSV\n",
    "    combined_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "folder_path = input(\"Enter the folder path of Canberra PDFs: \")  # Ask for folder path\n",
    "output_csv_path = input(\"Enter the output CSV file path (e.g. CORE_CanberraData_DATE.csv): \")  # Ask for the output file path\n",
    "extract_pdf_values(folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7568417",
   "metadata": {},
   "source": [
    "### Make note of which samples are missing data! This will be important when we plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ac233",
   "metadata": {},
   "source": [
    "# Open the output .csv file\n",
    "\n",
    "Check to make sure all radioisotope data translated correctly. \n",
    "\n",
    "# Create two new columns\n",
    "- ptsrc_pb210\n",
    "- ptsrc_pb210 error\n",
    "\n",
    "# Move Point Source Lead 210 data to ptsrc_pb210 and uncertainty to ptsrc_pb210 error of associated samples\n",
    "\n",
    "# CHECK the following\n",
    "\n",
    "Radioisotope data should have the following headings\n",
    "\n",
    " ### | File    | Pb-210   | Pb-210 error    | Bi-214  | Bi-214 error   | Pb-214    |Pb-214 error |  ptsrc_pb210    | ptsrc_pb210 error  | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f82fd",
   "metadata": {},
   "source": [
    "# Create a new .csv file from lab notebook for the sample weight data\n",
    "\n",
    "Sample weights should have the following headings: \n",
    "\n",
    "### | Core    | Top of interval (cm)   | Center point of interval    |Base of interval (cm)  | sediment weight (g)    | \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b5ac2",
   "metadata": {},
   "source": [
    "Run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776436fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Prompt user for file paths and output file name\n",
    "csv1_path = input(\"Enter the path to the sample weight CSV file (e.g., /path/weights.csv): \")\n",
    "csv2_path = input(\"Enter the path to the Canberra data CSV file (e.g., /path/canberra.csv): \")\n",
    "output_file_name = input(\"Enter the path for the output CSV file (e.g., CORE_AgeModel_DATE.csv): \")\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv(csv1_path)\n",
    "csv2 = pd.read_csv(csv2_path)\n",
    "\n",
    "# Extract 'Center point of interval' from .csv 2 based on the median of the last two digits in 'File'\n",
    "csv2['Center point of interval'] = csv2['File'].apply(\n",
    "    lambda x: np.median([int(num) for num in re.findall(r'\\d+', x.split('_')[-1])])\n",
    ")\n",
    "\n",
    "# Merge CSV files based on 'Center point of interval'\n",
    "data = pd.merge(csv1, csv2, on='Center point of interval', how='left')  # Merge using the new 'Center point of interval'\n",
    "\n",
    "# Now proceed with the calculations based on your updated requirements\n",
    "\n",
    "# Extract the 'year of core' as input\n",
    "year_of_core = int(input(\"Enter the year of core (e.g., 2023): \"))\n",
    "\n",
    "# Calculate columns\n",
    "data['Pb-210 activity (Bq/g)'] = data['Pb-210']/data['sediment weight (g)']\n",
    "data['Pb-210 correction factor'] = data['ptsrc_pb210'] / 151031.56  # Correction factor for Pb-210\n",
    "data['Self absorb. Corrected Pb-210 activity (Bq/g)'] = data['Pb-210 activity (Bq/g)'] / data['Pb-210 correction factor'] # Calculate Self absorb. Corrected Pb-210 activity (Bq/g)\n",
    "data['Bi-214 activity (Bq/g)'] = data['Bi-214'] / data['sediment weight (g)']\n",
    "data['Pb-214 activity (Bq/g)'] = data['Pb-214'] / data['sediment weight (g)']\n",
    "\n",
    "# Averaged supported activity of Bi-214 and Pb-214\n",
    "data['Averaged supported activity of Bi-214 and Pb-214 (Bq/g)'] = (\n",
    "    data['Bi-214 activity (Bq/g)'] + data['Pb-214 activity (Bq/g)']\n",
    ") / 2\n",
    "\n",
    "# Calculate background activity uncertainty (Bq/g)\n",
    "data['Background activity uncertainty (Bq/g)'] = (\n",
    "    (data['Bi-214 error'] + data['Pb-214 error']) / 2\n",
    ") / data['sediment weight (g)']\n",
    "\n",
    "\n",
    "# Calculate Excess Pb-210 (Bq/g)\n",
    "data['Excess Pb-210 (Bq/g)'] = data['Self absorb. Corrected Pb-210 activity (Bq/g)'] - data['Averaged supported activity of Bi-214 and Pb-214 (Bq/g)']\n",
    "\n",
    "# Calculate surface activity (value of first interval's 'Excess Pb-210 (Bq/g)')\n",
    "data['Surface activity'] = data['Excess Pb-210 (Bq/g)'].iloc[0]\n",
    "\n",
    "# Calculate Age bp\n",
    "data['Age bp'] = (1 / 0.03114) * np.log(data['Surface activity'] / data['Excess Pb-210 (Bq/g)'])\n",
    "\n",
    "# Calculate 'calendar years pre year of core'\n",
    "data['calendar years pre year of core'] = year_of_core - data['Age bp']\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "data.to_csv(output_file_name, index=False)  # Save as user-defined file name\n",
    "\n",
    "print(f\"Calculations completed, data exported to '{output_file_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d497d",
   "metadata": {},
   "source": [
    "# Check the output data. Make sure data isnt *fishy*\n",
    "Look at the column labeled 'Age'. Are the ages within the realm of possibility? If not, ask Asmara for help!\n",
    "\n",
    "# Now plot it!\n",
    "Run cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ccf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd  # Assuming you are using pandas for data handling\n",
    "\n",
    "# Get the core name from user input\n",
    "core_name = input(\"Enter the core name for the title: \")\n",
    "\n",
    "# Ask user for depths to label \"calendar years pre year of core\"\n",
    "depths_to_label_input = input(\"Enter the depths (comma-separated) where 'calendar years pre year of core' should be labeled (or type 'all' to label all intervals): \")\n",
    "\n",
    "# If the user types 'all', label all intervals\n",
    "if depths_to_label_input.lower() == 'all':\n",
    "    depths_to_label = data['Center point of interval'].tolist()  # Use all depth intervals\n",
    "else:\n",
    "    # Otherwise, parse the comma-separated depths\n",
    "    depths_to_label = [float(depth.strip()) for depth in depths_to_label_input.split(\",\")]\n",
    "\n",
    "# Ask user if there are intervals with undetectable radioisotopes\n",
    "missing_data_input = input(\"Are there any intervals with undetectable amounts of radioisotopes? (yes/no): \").strip().lower()\n",
    "\n",
    "if missing_data_input == 'yes':\n",
    "    # Ask for depths with undetectable radioisotopes\n",
    "    missing_depths_input = input(\"Enter the depths (comma-separated) with undetectable radioisotopes: \")\n",
    "    missing_depths = [float(depth.strip()) for depth in missing_depths_input.split(\",\")]\n",
    "else:\n",
    "    missing_depths = []\n",
    "\n",
    "# Define colors for the series and lighter shades for error bars\n",
    "excess_pb210_color = 'black'\n",
    "excess_pb210_error_color = mcolors.to_rgba(excess_pb210_color, alpha=0.3)  # Lighter black with transparency\n",
    "supported_activity_color = 'grey'\n",
    "supported_activity_error_color = mcolors.to_rgba(supported_activity_color, alpha=0.3)  # Lighter grey with transparency\n",
    "\n",
    "# Ask user where to save the plot PDF\n",
    "save_location = input(\"Enter the full path where you want to save the plot PDF (e.g., /path/to/your/directory/): \")\n",
    "plot_filename = f\"{core_name}_Age_Model.pdf\"  # Generate filename based on core name\n",
    "save_path = save_location + \"/\" + plot_filename\n",
    "\n",
    "# Plot 1: Dynamic Core 210Pb Uncorrected Activity\n",
    "plt.figure(figsize=(3, 5))\n",
    "plt.errorbar(data['Pb-214 activity (Bq/g)'], data['Center point of interval'], \n",
    "             xerr=data['Pb-214 error'], fmt='-', color=excess_pb210_color, label='Pb-214 activity (Bq/unit)', \n",
    "             capsize=5, linewidth=1, ecolor=excess_pb210_error_color)  # Lightened error color\n",
    "\n",
    "# Highlight missing depths with brown spans\n",
    "for y in missing_depths:\n",
    "    plt.axhspan(y - 0.5, y + 0.5, alpha=0.5, color='brown', label='Undetectable radioisotope' if y == missing_depths[0] else None)\n",
    "\n",
    "plt.title(f\"{core_name} 210 Pb Uncorrected Activity\", fontsize=18)\n",
    "plt.xlabel(\"Bq/g\", fontsize=14)\n",
    "plt.ylabel(\"Depth (cm)\", fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show depth from surface\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=1)  # Move legend below plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path, format='pdf', bbox_inches='tight')  # Save as PDF at user-specified location\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Dynamic Core 210Pb Activity with error bars\n",
    "plt.figure(figsize=(5, 10))\n",
    "\n",
    "# Series 1: Excess Pb-210 (Bq/g) with error bars\n",
    "plt.errorbar(data['Excess Pb-210 (Bq/g)'], data['Center point of interval'], \n",
    "             xerr=data['Pb-210 error'], fmt='-', color=excess_pb210_color, label='Excess Pb-210', \n",
    "             capsize=5, linewidth=1, ecolor=excess_pb210_error_color)  # Lightened error color\n",
    "\n",
    "# Series 2: Averaged supported activity of Bi-214 and Pb-214 (Bq/g) with error bars\n",
    "plt.errorbar(data['Averaged supported activity of Bi-214 and Pb-214 (Bq/g)'], data['Center point of interval'], \n",
    "             xerr=data['Background activity uncertainty (Bq/g)'], fmt='-', color=supported_activity_color, \n",
    "             label='Background Activity', capsize=5, linewidth=1, \n",
    "             ecolor=supported_activity_error_color)  # Lightened error color\n",
    "\n",
    "# Highlight missing depths with brown spans\n",
    "for y in missing_depths:\n",
    "    plt.axhspan(y - 0.5, y + 0.5, alpha=0.5, color='brown', label='Undetectable radioisotope' if y == missing_depths[0] else None)\n",
    "\n",
    "# Label the selected depths with \"calendar years pre year of core\" from the data column\n",
    "for i, depth in enumerate(data['Center point of interval']):\n",
    "    if depth in depths_to_label:\n",
    "        # Get the corresponding 'calendar years pre year of core' value and convert to an integer for a clean label\n",
    "        year_value = data['calendar years pre year of core'].iloc[i]\n",
    "        \n",
    "        # Check if the value is NaN (and skip it if it is)\n",
    "        if not pd.isna(year_value):\n",
    "            year = int(year_value)\n",
    "            # Add an offset to move the label slightly to the right (along the x-axis)\n",
    "            plt.text(data['Excess Pb-210 (Bq/g)'].iloc[i] + 0.05, depth,  # Add 0.1 to shift the label to the right\n",
    "                     f'{year}', fontsize=14, color='black', verticalalignment='center')\n",
    "\n",
    "plt.title(f\"{core_name} Age Model\", fontsize=18)\n",
    "plt.xlabel(\"Bq/unit\", fontsize=14)\n",
    "plt.ylabel(\"Depth (cm)\", fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show depth from surface\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=1)  # Move legend below plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path, format='pdf', bbox_inches='tight')  # Save as PDF at user-specified location\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281d3c7",
   "metadata": {},
   "source": [
    "# Well done!\n",
    "\n",
    "#### When you've finished, go to Cell > All Output > Clear to be ready for the next user of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
